import redis.asyncio as redis  # type: ignore # Redis client for async operations
from fastapi import FastAPI, BackgroundTasks, HTTPException, Header  # type: ignore
from pydantic import BaseModel  # type: ignore # For input validation
from uuid import uuid4  # To generate unique IDs for predictions
import asyncio  # For handling asynchronous operations
import random  # For generating random numbers (simulating prediction results)
import json  # For working with JSON data
import os  # For handling environment variables

# Initialize FastAPI app
app = FastAPI()

# Redis configuration: Set Redis host and port, defaulting to localhost and port 6379
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

# Connect to Redis (async client)
r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)


# Endpoint for favicon request, returns a simple message for requests to /favicon.ico
@app.get("/favicon.ico")
async def favicon():
    return {"message": "No favicon"}


# Simulate a mock model prediction function with async behavior
async def mock_model_predict(input: str):
    # Simulate model processing time with sleep (15 seconds)
    await asyncio.sleep(15)
    # Return a random result as if generated by the model
    return {"input": input, "result": str(random.randint(1000, 20000))}


# Pydantic model for the input to the prediction endpoint
class PredictInput(BaseModel):
    input: str  # Expecting a string as input for the prediction


# Background task function that performs the prediction and stores the result in Redis
async def background_predict(input: str, prediction_id: str):
    print(f"\nStarting background prediction for {prediction_id}...")

    # Call the mock model prediction function
    result = await mock_model_predict(input)
    print(f"Completed prediction {prediction_id}. Result: {result}")

    # Store the result in Redis with a TTL of 1 hour (3600 seconds)
    await r.setex(prediction_id, 3600, json.dumps({
        "status": "completed",  # Mark as completed
        "input": input,
        "result": result["result"]
    }))
    print(f"Prediction {prediction_id} stored in Redis with result.\n")


# Root endpoint that simply returns a welcome message
@app.get("/")
def root():
    return {"message": "Welcome to the Mock Model API!"}


# Prediction route for Async and Sync processing
@app.post("/predict")
async def predict(
    request: PredictInput,
    background_tasks: BackgroundTasks,
    async_mode: str = Header(None)  # Retrieve async_mode from request headers
):
    # Log the async_mode value
    print(f"\nReceived prediction request with async_mode: {async_mode}")

    if async_mode and async_mode.lower() == "true":
        # Asynchronous processing
        prediction_id = str(uuid4())  # Generate a unique prediction ID

        # Log the start of async processing
        print(f"Processing prediction {prediction_id} asynchronously.")

        # Set initial prediction status as 'processing' in Redis
        await r.setex(prediction_id, 3600, json.dumps({
            "status": "processing",  # Mark status as processing
            "input": request.input  # Store the input
        }))
        print(f"Prediction {prediction_id} marked as 'processing' in Redis.\n")

        # Add background task to process the prediction asynchronously
        background_tasks.add_task(
            background_predict, request.input, prediction_id)

        return {"message": "Request received. Processing asynchronously.", "prediction_id": prediction_id}

    else:
        # Synchronous processing (default if async_mode is not specified or set to False)
        print(
            f"Processing prediction synchronously for input: {request.input}")
        result = await mock_model_predict(request.input)

        # Log the synchronous result
        print(
            f"Prediction result for input '{request.input}': {result['result']}\n")

        # Return the result immediately
        return {"input": request.input, "result": result["result"]}


# Endpoint to check the status of a prediction using the prediction ID
@app.get("/predict/{prediction_id}")
async def get_prediction_result(prediction_id: str):
    # Log the request to get prediction status
    print(f"Checking status for prediction ID: {prediction_id}")

    # Retrieve the prediction status from Redis using the prediction ID
    prediction = await r.get(prediction_id)

    # If the prediction ID does not exist in Redis, raise a 404 error
    if not prediction:
        print(f"Prediction ID {prediction_id} not found.")
        raise HTTPException(status_code=404, detail="Prediction ID not found.")

    # Parse the retrieved JSON data
    prediction = json.loads(prediction)

    # If the prediction is still processing, return a 400 status code
    if prediction["status"] == "processing":
        print(f"Prediction ID {prediction_id} is still processing.")
        raise HTTPException(
            status_code=400, detail="Prediction is still processing.")

    # If the prediction is completed, return the result
    if prediction["status"] == "completed":
        print(
            f"Prediction ID {prediction_id} is completed. Result: {prediction['result']}\n")
        return {"prediction_id": prediction_id, "status": "completed", "output": prediction}

    # If there is any unexpected error, raise a 500 error
    print(f"Unexpected error occurred with prediction ID {prediction_id}.")
    raise HTTPException(status_code=500, detail="Unknown error occurred.")
